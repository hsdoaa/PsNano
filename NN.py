#to Get Reproducible Results with Keras #https://machinelearningmastery.com/reproducible-results-neural-networks-keras/
from numpy.random import seed
seed(1)
import tensorflow
tensorflow.random.set_seed(1234)
####################################################
from sklearn.preprocessing import OneHotEncoder
from sklearn import preprocessing
import pandas as pd
from statistics import mean, median
import numpy as np
import sys
import signalExtractor as signal
import pseudoExtractor as ps
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler #For feature normalization
scaler = MinMaxScaler()


#get signal length:
def get_signal_length(x):  
    return len(x)

#start pseudoExtractor 
controlHela, pseudoHela = ps.get_Hela()
#omit file name
drp = [0, 2]
controlHela = controlHela.drop(drp, axis=1) 
pseudoHela = pseudoHela.drop(drp, axis=1)

print(controlHela.iloc[0,1])

kmerData = []
for i in range(len(controlHela)):
    kmer = controlHela.iloc[i, 0] 
    kmerData.append([kmer])

    values = controlHela.iloc[i, 1]  

    sig = ""
    for j in range(len(values)):
        if values[j] == '_':
            #convert to int
            kmerData[i].append(int(sig))
            sig = ""

        elif j == (len(values) - 1):
            sig += values[j]
            kmerData[i].append(int(sig))
            sig = ""

        else:
            sig += values[j]
        


pseudoKmerData = []
for i in range(len(pseudoHela)):
    kmer = pseudoHela.iloc[i, 0]
    pseudoKmerData.append([kmer])

    values = pseudoHela.iloc[i, 1]
    sig = ""
    for j in range(len(values)):
        if values[j] == '_':
            #convert to int
            pseudoKmerData[i].append(int(sig))
            sig = ""

        elif j == (len(values) - 1):
            sig += values[j]
            pseudoKmerData[i].append(int(sig))
            sig = ""

        else:
            sig += values[j]

X = []
Xval = []
Y = []
Yval = []


prevIndexes = np.random.choice(len(controlHela), 360, replace=False)
kmerData = np.array(kmerData)[prevIndexes]
print("size of ", len(kmerData))
total = 360 + len(pseudoHela)
indexes = np.random.choice(total, total, replace=False)    


for i in range(len(kmerData)):
    X.append(kmerData[i][0])


for i in range(len(pseudoKmerData)):
    X.append(pseudoKmerData[i][0]) # Now X stores kmerData and pseudoKmerData


#onehot encoding of kmer data
le = preprocessing.LabelEncoder()          
le.fit(X)
print(le.classes_)
X = le.transform(X)
X = X.reshape(-1, 1)

#onehot encode
_, n_features = np.shape(X)
print("&&&&&",n_features)
enc = OneHotEncoder(handle_unknown='ignore',categories=[range(350)]*n_features)# note we replace n_values=350 which is depricated in vrsion 0.2 with categories=[range(350)]*n_features
enc.fit(X)
onehots = enc.transform(X).toarray()
X = onehots

##feature extrextion from unmodified extracted Nanopore signal 
allKmerData=[]
for i in range(len(kmerData)):
    #orginal signal without padding
    allKmerData.append(kmerData[i][1:]) #store rows
    Xval.append([mean(kmerData[i][1:]), median(kmerData[i][1:]), max(kmerData[i][1:]), min(kmerData[i][1:]),get_signal_length(kmerData[i][1:])])
    Yval.append([0])

##feature extrextion from modified extracted Nanopore signal 
for i in range(len(pseudoKmerData)):
    allKmerData.append(pseudoKmerData[i][1:])
    Xval.append([mean(pseudoKmerData[i][1:]), median(pseudoKmerData[i][1:]), max(pseudoKmerData[i][1:]), min(pseudoKmerData[i][1:]),get_signal_length(pseudoKmerData[i][1:])])
    Yval.append([1])



#combine onehot encoding of kemer features with features extracted from nanopore signal
for i in range(len(Xval)):
    for j in range(len(X[i])):
        Xval[i].append(X[i][j])


#randomize indexes
X = np.array(Xval)[indexes]                   
#X = np.array(X)[indexes]    #when X has onehot encoding of kemer features only 
Y = np.array(Yval)[indexes]
print("pppppppppp",X.shape)
print("jjjjjj",Y.shape)
print("xxxxxxxxxxxxx",type(X))
print("MMMMMMMMMMMMMMM",type(Y))
print(len(X), len(Y))
print("************************",X[0])
print("-------------",Y[0])
a,b=X.shape
########################################################
#To get nanopore signal intenesity 
#get padded signal data
x= signal.signal_data(allKmerData)
print("xxxxxxxxxxxxx",type(x))
print(len(x))
print("##########",x[0])
#concatenate signal intensity features with other features extracted from extracted Nanopore signal
X1=np.concatenate((X,x),axis=1)
print("OOOOOOOOOOOOOOOOOO",X1)

####################################################define the NN model in keras##############################################

# Evaluate the model: Model Accuracy, how often is the classifier correct
from numpy import loadtxt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense 
from tensorflow.keras.optimizers import SGD
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation
from sklearn.metrics import classification_report #for classifier evaluation
from sklearn.metrics import roc_auc_score # for printing AUC
#from sklearn.metrics import roc_auc_score # for printing AUC
from sklearn.metrics import roc_curve, auc


# To set seed random number
from numpy.random import seed
seed(4)
import tensorflow
tensorflow.random.set_seed(4)

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)


#scale training data
X_train= scaler.fit_transform(X_train) #https://stackoverflow.com/questions/41560021/numpy-ndarray-object-has-no-attribute-values
#scale testing data
X_test= scaler.fit_transform(X_test)

model = Sequential()
model.add(Dense(12, input_dim=b, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# compile the keras model
#model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.05, momentum=0.99), metrics=['accuracy'])
model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])

# Fit the model                            #epochs= Ô¨Åxed number of iterations through the dataset called epochs
#model.fit(X_train, y_train, validation_split=0.2, epochs=150, batch_size=16) #batch_size=the number of instances that are evaluated before a weight update
history=model.fit(X_train, y_train, validation_split=0.2, epochs=150, batch_size=len(X_train))

# evaluate the keras model on the same dataset, but the dataset can be devided into training and testing, then fit the model on the training and evalaute the model on testing
_, accuracy_train = model.evaluate(X_train, y_train, verbose=1)
_, accuracy_test = model.evaluate(X_test, y_test, verbose=1)
print('Accuracy on training: %.2f' % (accuracy_train*100))
print('Accuracy on testing: %.2f' % (accuracy_test*100))

#needed for plotting learning curve
train_loss = history.history['loss']
val_loss   = history.history['val_loss']
train_acc  = history.history['accuracy']
val_acc    = history.history['val_accuracy']
xc         = range(50)


#plot epochs versus accuracy curve.

plt.plot(train_acc, label="Training accuracy")
plt.plot(val_acc, label="Validation accuracy")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc="best")
plt.show()
'''
plt.plot(train_loss, label="Training loss")
plt.plot(val_loss, label="Validation loss")
plt.xlabel('Epochs')
plt.ylabel('loss')
plt.legend(loc="best")
plt.show()
'''

# make class predictions with the model
predictions = model.predict_classes(X_test)
y_pred = model.predict_classes(X_test)
y_prob = model.predict_proba(X_test)
#y_prob = y_prob[:,1]

print(classification_report(y_test, y_pred))
auc=roc_auc_score(y_test.round(),y_pred)
auc = float("{0:.3f}".format(auc))
print("AUC=",auc)

print("X 4Features+length before that was X1 features for 4 types of X featured concatenated with x")


fpr, tpr, thresholds = metrics.roc_curve(y_test, y_prob)


# Print ROC curve
plt.plot(fpr,tpr)
plt.title("ROC Curve")
# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.show() 


